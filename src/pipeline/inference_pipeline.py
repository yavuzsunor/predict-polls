import sys
import os
import numpy as np

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

from zemberek import (TurkishMorphology,
                      TurkishSentenceNormalizer,
                      TurkishSpellChecker,
                      )
import snscrape.modules.twitter as sntwitter
import eksipy
import asyncio

from src.exception import CustomException
from src.utils import clean_text
from src.components.model_trainer import ModelTrainerConfig
from src.components.data_transformation import DataTransformationConfig 

morphology = TurkishMorphology.create_with_defaults()
normalizer = TurkishSentenceNormalizer(morphology)
label_sentiment = sentiment_dict = {0:"kÄ±zgÄ±n",
                                    1:"korku",
                                    2:"mutlu",
                                    3:"surpriz",
                                    4:"Ã¼zgÃ¼n"}
test_tweets = [
    'Ã–zgÃ¼r ve temiz bir turkiye icin ðŸ‡¹ðŸ‡·\n#SanaSoz \n#SanaSozBaharlarGelecek \n#cezevindenseskaydi \n#bizdendesanasoz \n#kilicdaroglu \n#GenelAf \n#infazduezenlemesi \n#ADALET \n#af \n#MilletTarihYazacak \n#MilletÄ°ttifakÄ± https://t.co/9hhKFNftGs',
    'Ä°ÅŸte bu! TÃ¼rkiyeâ€™me yeni bir nefes #kÄ±lÄ±Ã§daroÄŸlu https://t.co/APxXlsvV4l',
    'MÃ¼slÃ¼manÄ±zElhamdÃ¼rillah\nkalkÄ±p da terÃ¶risÄ°Tlerin desteklediÄŸi yalamalara OY verirmiyiz\nkatiLLErin ittifakÃ§Ä±larÄ±na OY verirmiyiz\nÄ°nsanÄ±z Biz,beynimiz var Allah akÄ±lfikir ihsan eylemiÅŸ\nanguSdeÄŸilik\nKuranÄ±kerim yakan bebek katillerine OY istiyor kasetci #KÄ±lÄ±Ã§daroÄŸlu  ve adaylardaðŸ‘‡ https://t.co/n5hAsxKRqb',
    'Åžimdi #HaberGlobal; Ä°Ã§iÅŸleri bakanÄ± #Soylu, \n-#DemirtaÅŸ\'Ä±n #PKK\'ya "silah bÄ±raktÄ±rma" Ã§Ä±kÄ±ÅŸÄ± Ã¶rgÃ¼te nefeslenme numarasÄ±dÄ±r, diyor.\nBu doÄŸru deÄŸil..\nPKK ilk defa #Sevr\'deki bÃ¶lÃ¼nme projesi\'ni #KÄ±lÄ±Ã§daroÄŸlu ve #MilletittifakÄ± yapÄ±lanmasÄ±yla gÃ¼Ã§lÃ¼ olrk yakalamÄ±ÅŸtÄ±r, buna oynuyor https://t.co/ppiazSatHI',
    'Ä°hanetin arkasÄ±nda #emperyalizm ile iÅŸbirliÄŸi yapan #TÃ¼rksolu var.\n-09 temmuz 1934 1.KÃ¼rdoloji kongresi"\n-1955 Moskova KomÃ¼nist yazarlar kongresi\n-1978 Lice, Fis kÃ¶yÃ¼ #PKK\'nÄ±n kuruluÅŸu.\nBu iÄŸrenÃ§ sol, #KÄ±lÄ±Ã§daroÄŸlu ile harekete geÃ§ti.. https://t.co/feD6TwpwiF',
    'Ne diyor #KÄ±lÄ±Ã§daroÄŸlu; "#KÃ¼rtkimliÄŸi\'ni meclise kabul ettireceÄŸim, #yerelyÃ¶netimler yasasÄ±nÄ± Ã§Ä±karacaÄŸÄ±m". Bu sinsi bir Sevr Ã§Ä±kÄ±ÅŸÄ±dÄ±r\n#Sevr\'i imzlayanlar "vatan haini" ilan edilmiÅŸtir.\nBu ihaneti #AtatÃ¼rk ve arkadaÅŸlarÄ± "KurtuluÅŸ savaÅŸÄ±" sonrasÄ± #Lozan\'da yÄ±rtÄ±p atmÄ±ÅŸlardÄ±r. https://t.co/JJJWQ2M8kW',
    'KÄ±lÄ±Ã§daroÄŸlu Bu Sefer Ã‡ok GÃ¼Ã§lÃ¼. #kÄ±lÄ±Ã§daroÄŸlu #chp https://t.co/JgSoLgI1da @YouTube aracÄ±lÄ±ÄŸÄ±yla',
    "@__KESAFET64__ #KÄ±lÄ±Ã§daroÄŸluâ€™na oy yokðŸ‘ŽðŸ»",
    '#MuhammetYakut #Tether\n#kilicdaroglu #ibbguvenligizambekliyor %4 zam %300 enflasyon bu gidiÅŸ nereye',
    "KÄ±lÄ±Ã§daroÄŸlu 'Alevi' notuyla paylaÅŸtÄ±ÄŸÄ± videoda ilk kez oy kullanacak genÃ§lere seslendi \nhttps://t.co/nFj6WBQhxT \n#kÄ±lÄ±Ã§daroÄŸlu #alevi https://t.co/l63u311gYD",
    'Optimar, Ä°lk kez oy kullanacak genÃ§lerin nabzÄ±nÄ±n tutulduÄŸu anketinin sonuÃ§larÄ±nÄ± yayÄ±nladÄ±:   ðŸ“· % 51,2 ErdoÄŸan,  ðŸ“· % 39,2 KÄ±lÄ±Ã§daroÄŸlu   ðŸ“· % 7,4 Muharrem Ä°nce  ðŸ“· % 1,1 Sinan OÄŸan  \n\n#anket #optimar #seÃ§im2023 #erdoÄŸan #kÄ±lÄ±Ã§daroÄŸlu #ince #oÄŸan https://t.co/NE2ORt22E4',
]


class Inference:
    def __init__(self):
        self.model_trainer_config = ModelTrainerConfig()
        self.data_transformation_config = DataTransformationConfig() 
        self.saved_tokenizer = AutoTokenizer.from_pretrained(self.model_trainer_config.model_data_path)
        self.saved_model = AutoModelForSequenceClassification.from_pretrained(self.model_trainer_config.model_data_path)
    
    def scrape_twitter(self, keyword):
        """
        There should be a twitter scraper code that needs to run during the inference.
        It should return some results like percentage-wise positive negative ratio for a specific keyword.
        It should also save the scraped tweets(maybe as csv) and their inference results(could be a json file) somewhere.
        """
        try:
            scraper = sntwitter.TwitterSearchScraper(keyword, top=True)
            raw_tweets = []
            clean_tweets = []
            for i, tweet in enumerate(scraper.get_items()):
                data = [tweet.date,
                        tweet.id,
                        tweet.rawContent,
                        tweet.user.username]
                raw_tweets.append(data) #TODO: need to store the original tweets in somewhere
                clean_tweets.append(normalizer.normalize(clean_text(data)))
                if i > 2:
                    break

            print(raw_tweets, "\n")
            print("\n\n")
            print(clean_tweets, "\n")
        
        except Exception as e:
            raise CustomException(e, sys) 

    def scrape_eksisozluk(self, search_topic, entry_list=[]):
        search_topic = search_topic
        try:
            async def getTopic():
                eksi = eksipy.Eksi()
                topic = await eksi.getTopic(search_topic)
                for page in range(6171, 6173):
                    entrys = await topic.getEntrys(page=page)
                    for entry in entrys:
                        # print("*" * 10)
                        # print(entry.text())
                        entry_list.append(entry.text())
                #         print(entry.author.nick)
                #         print(entry.date)

                    # print("*" * 10)

            loop = asyncio.get_event_loop()
            loop.run_until_complete(getTopic())

            return entry_list

        except Exception as e:
            raise CustomException(e, sys)

    def predict_sentiment(self, tweets, model):
        try:
            encodings = self.saved_tokenizer.batch_encode_plus(tweets,
                                                                max_length=64,
                                                                add_special_tokens=True,
                                                                return_attention_mask=True,
                                                                pad_to_max_length=True,
                                                                truncation=True)

            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            model.to(device)
            model.eval()

            test_inputs = torch.tensor(encodings["input_ids"]).to(device)
            test_masks = torch.tensor(encodings["attention_mask"]).to(device)
            with torch.no_grad():
                predictions = model(test_inputs, attention_mask=test_masks)
            
            logits = predictions[0].detach().numpy()
            return np.argmax(logits, axis=1).flatten()

        except Exception as e:
            raise CustomException(e, sys)

if __name__ == "__main__":

    inference = Inference()
    # inference.scrape_twitter('#kilicdaroglu') 
    eksi_entries = inference.scrape_eksisozluk('kemal kÄ±lÄ±Ã§daroÄŸlu')

    clean_eksi_entries = []
    for entry in eksi_entries:
        clean_entry = normalizer.normalize(clean_text(entry))
        # print(clean_entry, "\n")        
        clean_eksi_entries.append(clean_entry)


    predicted_labels = inference.predict_sentiment(clean_eksi_entries, inference.saved_model)

    for orig_entry, final_entry, predicted_label in zip(eksi_entries, clean_eksi_entries, predicted_labels):
        # Print the original tweet
        print(orig_entry, '\n')
        # Print the final clean tweet
        print(final_entry, '\n')
        # # Print the sentence split into tokens.
        # print('Tokenized: ', inference.saved_tokenizer.tokenize(tweet))
        # # Print the sentence mapped to token ids.
        # print('Token IDs: ', inference.saved_tokenizer.convert_tokens_to_ids(inference.saved_tokenizer.tokenize(tweet)))
        print(label_sentiment[predicted_label], '\n\n')
